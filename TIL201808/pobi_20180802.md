# 성능개선 이슈

1. foward proxy vs reverse proxy
2. (DB) master / slave -> read/write (active/stand by)
3. server cache 데이터 가진다의 의미?
4. cache server의 위치, was cache
5. L4 -> L7이 분명 더 좋은데, 왜 L7으로 가지않고 L4를 쓰는 이유는?
6. MQ -> kafka MQ (Queue 에 담은 message 의미?)
7.

---
1. foward proxy / reverse proxy 는 역할이름이다. 실제 apache나 nginx를 사용해서 proxy서버를 구성한 뒤, proxy역할을 하게 만드는것이고, 프록시서버 혹은 캐시서버 어떻게 불러도 상관없다.

(1) client | (2) internet | (3) server

foward proxy |   <--->   | reverse proxy

1) foward proxy 사용 이유

- server 에 접근하지 않고 client단에서 정보를 넘겨줄 수 있다.
첫 유저의 접근(server로 접근한다.) 이후부터는 caching 해서 foward proxy에서 바로 넘겨준다.

2) reverse proxy 사용 이유

- 서버측 가장 앞단에서 성능/보안 등의 이슈를 caching 해놓았다가, web server에 요청을 보내지 않고, 바로 유저에게 접근해준다.
- DDOS공격을 web server 앞단의 reverse proxy server에서 담당하게 된다면, web server는 이 이슈를 고려하지 않아도 되고, 결과적으로 방어할 수 있게 된다.

- 보안적으로 봤을 때, reverse proxy서버는 실제 was의 주소를 모르게 가려주는 역할도 있다. -> 클라이언트는 실제 최종 목적지를 모른다.

3) 실제 입사 후에, foward proxy를 세팅할 일은 없을것이다. reverse proxy는 간혹 세팅을 할 수 있다.

> 프록시 서버의 역할에 대해서 추가 학습이 필요함.

---
2. master slave (DB)

장애가 났을 때, 빠른 시간내에 복구할 수 있는가에 대한 전략으로 나온 것이다.

데이터베이스 서버가 다운되었다. 어떻게 빨리 서버를 복구할 수 있을까?

master <- tomcat    slave

tomcat은 기본적으로 master를 바라보게 되어있다.

master가 다운되었을 때, tomcat이 바로 slave를 바라보게 만들면 빠르게 대처된다. 그러려면, master와 slave 안에 있는 데이터베이스의 내용이 항상 같아야한다. 이를 달성하려면, master에서 주기적으로 slave에 sync를 맞춰줘야(복제해줘야) 한다.

이러한 상황에서 master가 active가 되는것이고, slave가 stand by가 되는 것이다.

데이터를 쓰는것보다 읽는양이 훨씬 많은 서비스가 있다고 가정하자.
이 경우에는

db1 <-------------> db1' (sync된 db)
|                  /
|write            / read
|update          /
|               /
tomcat---------/

db1이 장애가 나면, update, write는 안되지만, read(조회)는 가능하다. 이런식의 구성을 하는것이 좋다. (naver가 이런 구성이 많다. 워낙 조회가 중요하니..) 이러한 경우는 db1, db1' 가 active active 구조라고 할 수 있다.

---
db sharding 란?

하나의 테이블에 너무 많은 정보를 가지고있으면, select할 때, 성능이 많이 걸린다. 모든 서버의 요청이 한 데이터베이스로 모이게되면 성능이 떨어지는것은 당연하다. 이를 방지하기 위해 database를 분산시켜 놓는것을 db sharding이라고 한다. n대로 db를 확장하지만, db schema는 모두 동일하다.

보통 sharding을 할 때, 2^n 개수로 sharding한다. 기존에 2대로 시작하고, 이후에 4대, 8대 ... 와 같은 방식으로 sharding한다는 뜻.

네이버는 블로그 서비스를 유지하기 위해 몇개의 db로 sharding을 할까?

블로그 id값을 기준으로 나눠진다. 즉 8대의 db가 있다고 하면, 블로그 id값을 8로 나눈 나머지를 통해 db를 바라보게 한다. (0~7)

8대의 db안에 공통으로 관리할 데이터가 있다면, common db를 만들어서 이 db 하나로 통합관리한다.

sharding의 문제점은 균등 분할이 안된다. 예를 들자면, 중고나라 라는 까페가 다른 네이버 까페 몇개정도의 트래픽을 가질까? 중고나라는 최소 수천개~만개 정도의 트래픽을 발생시킨다. 이것이 바로 sharding의 문제이다. 다른 서버는 널널한데, 중고나라 서비스를 담당하는 db만 성능 이슈가 발견되는 것이다.

까페 로직 곳곳에 if문이 있는데, 이게 무엇일까?
> 중고나라이면 예외처리를 한 것이다. 안그러면 성능이슈가 발생하기 때문이다. 현재는 어떨지 모르겠지만, 예전엔 그랬다.

페이스북이나 구글은 이러한 방법을 사용해서 처리할까?

아니다. sharding은, 관계형 데이터베이스일때, 가장 쉽게 확장을 할 수 있는 설계 전략이 있다. 페이스북이나 구글은 RDB를 사용하지 않고 NoSQL등을 사용한다. 따라서, 데이터 성격에 따라서 적합한 db를 나누는 방법을 사용한다.

naver가 2000년 초중반의 유명한 모델을 사용했다면, 구글이나 페이스북은 최근 트렌드의 모델을 사용하는 것이다.

---
3. server cache 의미?
4.

ws      |   WAS  |   DB

NginX   |   TOMCAT

apache  |

cache는 아주 많은곳에서 쓰이고 있다. 모든 데이터를 DB에 요청하게 되면, 성능이 느려지는것은 당연하다. 때문에 자주 사용되는 데이터들은 WAS가 데이터를 가지고 있다가 바로 전달해주면 100~1000배이상 성능이 좋아진다. 즉, cache를 잘 쓰면, 성능을 폭발적으로 향상시킬수있다.

WAS서버는 JVM에 일정량의 RAM 메모리 영역을 할당한다. 이 RAM 메모리상에 저장해두었다가 DB요청 없이 곧바로 전달해주는 것이다. 이것이 WAS 의 local cache 구조이다.

이 local cache의 문제점은, n대의 WAS마다 똑같은 데이터를 계속 캐싱하고 있을 수 있다는것. 메모리가 부족해서 WAS가 본래 해야 할 서비스에 차질이 생기는 것. 이렇게 두가지가 대표적이다.

RDB는 cache를 관리하기에 좋은 데이터베이스가 아니다. 따라서 별도의 cache server를 두고,

WAS -> RDB 요청 이후에 WAS 내부 local cache에 캐싱하지 않고, 그 데이터를 cache server에 저장한다. 이후 같은 요청이 올 때마다, cache server에 요청을 해서 받아오는 방법이 적합하다. 이런식으로 cache를 구성하는것을 global cache라고 한다.(cache server를 global cache)

JAVA spring에서는 @Cacheable 어노테이션으로 어디에 캐싱을 할 지 설정할 수 있다. AOP로 구성할 수 있는 개념.

캐시전략을 세울때 중요한것은 캐시의 히트율이다. 데이터를 캐싱했는데 이 데이터가 얼마나 자주 사용되는가? 가 중요하다는 것이다. 과거의 데이터들은 사용자들이 잘 접근하지 않는다.
한 예로 2페이지 넘어가도록 글을 읽은적이 많은가? 아니다. 1페이지 2페이지 내에서 왠만하면 조회를 한다. 이러한 히트율을 따져보는것이 캐싱전략을 짜는 의미있는 방법이다.

---
5. L7 비싸다..

좋은점은 어플리케이션 계층까지 볼 수 있고,

sticky session - tomcat서버가 1~n개 있으면, 한번 1번 서버로 요청을 했다면, 이후에도 계속해서 같은 tomcat서버에 요청을 보내도록 해준다. 이는 쿠키를 이용해서 가능하고, L7 로드밸런싱 장비를 이용해서 가능하다. 쿠키는 application 계층(7번계층) 을 이용하는 것이기 때문에, L7만 가능.

서비스가 더 커지면, 별도의 session 관리 서버를 둔다.(redis등으로) 그것이 바로 session clusting이다.

최근에는 L4, L7장비를 open source software로 사용할 수 있다. 대표적인 것은 HAProxy이다.

---
6. MQ -> kafka

Queue | key:value

MQ (Message Queue)

Queue 자료구조의 특징 - 선입 선출

WAS 내부에 200개의 멀티스레드가 default이고, 이 이후로 들어오는 요청들은 queue에 쌓인다.
(모든 서버의 앞단에는 queue가 있다.)

그렇다면 메시지 큐는 무엇인가?

앞단에 tomcat 서버가 많고

이메일 보내기 라는 기능이 있다. 이메일을 보낼 때, 로그인을 하고 이메일을 보낸다고 하자. 로그인에 0.5초, 이메일 보내기에 2초가 걸리다 하면, 요청은 최소 2.5초 이후에 발생한다. 이를 비동기 처리하여 시간을 단축시키고 싶을때, 로그인 0.5초 후에 사용자에게 로그인이 잘 되었다고 보내주면서, 비동기로 동시에 이메일을 보낸다.

이러한 비동기가 많이 쌓이면, WAS에 많은 부담이 가게 된다. 이를 한곳에 몰아주어서 이 비동기 처리만 담당하는 서버를 만들어 주고싶다. 이 때에 사용하는 서버가 MQ서버이다. 실제로 이메일은 10초 후에 보내질 수도 있다. 이처럼 바로 적용되야 하는 사항이 아닌것은(그것이 그렇게 중요사항이 아닌 서비스들은) MQ서버로 보내서 비동기 처리하면 되는것이다.

kafka는 MQ의 한 종류이다. 최근의 트렌드는 kafka이다. (최근 급부상)

캐시서버가 redis가 급부상 하고있는 것처럼.
